{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from vector_quantize_pytorch import VectorQuantize\n",
    "\n",
    "vq = VectorQuantize(\n",
    "    dim = 256,\n",
    "    codebook_size = 512,     # codebook size\n",
    "    decay = 0.8,             # the exponential moving average decay, lower means the dictionary will change faster\n",
    "    commitment_weight = 1.   # the weight on the commitment loss\n",
    ")\n",
    "\n",
    "x = torch.randn(1, 1024, 256)\n",
    "quantized, indices, commit_loss = vq(x) # (1, 1024, 256), (1, 1024), (1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from options import Options\n",
    "from archs.run_series import run_series\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from archs.network import GAT, Transform\n",
    "\n",
    "from archs.distractors import select_distractors, select_fixed_distractors\n",
    "\n",
    "class SenderRel(nn.Module):\n",
    "    def __init__(self, num_node_features, embedding_size, heads, layer, hidden_size, temperature):\n",
    "        super(SenderRel, self).__init__()\n",
    "        self.num_node_features = num_node_features\n",
    "        self.heads = heads\n",
    "        self.hidden_size = hidden_size\n",
    "        self.temp = temperature\n",
    "          \n",
    "        self.layer = Transform(self.num_node_features, embedding_size, heads) if layer == 'transform' else GAT(self.num_node_features, embedding_size, heads) \n",
    "        self.fc = nn.Linear(2 * embedding_size, hidden_size) \n",
    "\n",
    "    def forward(self, x, _aux_input):\n",
    "        data = _aux_input\n",
    "\n",
    "        batch_ptr, target_node_idx, ego_idx = data.ptr, data.target_node_idx, data.ego_node_idx\n",
    "\n",
    "        h = self.layer(data)\n",
    "\n",
    "        adjusted_ego_idx = ego_idx + batch_ptr[:-1]\n",
    "        adjusted_target_node_idx = target_node_idx + batch_ptr[:-1]\n",
    "  \n",
    "        target_embedding = torch.cat((h[adjusted_target_node_idx], h[adjusted_ego_idx]), dim=1) \n",
    "\n",
    "        output = self.fc(target_embedding)   \n",
    "\n",
    "        return output # batch_size x hidden_size\n",
    "\n",
    "class ReceiverRel(nn.Module):\n",
    "    def __init__(self, num_node_features, embedding_size, heads, layer, hidden_size, distractors):\n",
    "        super(ReceiverRel, self).__init__()\n",
    "        self.num_node_features = num_node_features\n",
    "        self.heads = heads\n",
    "        self.distractors = distractors\n",
    "        \n",
    "        self.layer = Transform(self.num_node_features, embedding_size, heads) if layer == 'transform' else GAT(self.num_node_features, embedding_size, heads)\n",
    "        self.fc = nn.Linear(hidden_size, embedding_size)\n",
    "\n",
    "    def forward(self, message, _input, _aux_input):\n",
    "        data = _aux_input\n",
    "        h = self.layer(data)\n",
    "\n",
    "        print(data.ego_node_idx)\n",
    "        print(data.ego_node)\n",
    "        # indices, _ = select_distractors(\n",
    "        #     data, \n",
    "        #     self.distractors if not getattr(data, 'evaluation', False) else len(data.target_node) - 1,\n",
    "        #     evaluation=getattr(data, 'evaluation', False)\n",
    "        # )\n",
    "\n",
    "        indices, _ = select_fixed_distractors(\n",
    "            data, \n",
    "            self.distractors if not getattr(data, 'evaluation', False) else len(data.target_node) - 1,\n",
    "            evaluation=getattr(data, 'evaluation', False)\n",
    "        )\n",
    "\n",
    "        embeddings = h[indices]\n",
    "\n",
    "        batch_size = data.num_graphs\n",
    "        num_candidates = embeddings.size(0) // batch_size\n",
    "\n",
    "        embeddings = embeddings.view(batch_size, num_candidates, -1)\n",
    "        message = self.fc(message)\n",
    "        message = message.unsqueeze(2)  \n",
    "\n",
    "        dot_products = torch.bmm(embeddings, message).squeeze(-1)  \n",
    "        log_probabilities = F.log_softmax(dot_products, dim=1)\n",
    "\n",
    "        # add small random noise\n",
    "        log_probabilities = log_probabilities + 1e-10 * torch.randn_like(log_probabilities)\n",
    "        \n",
    "        return log_probabilities"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "kinship",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
