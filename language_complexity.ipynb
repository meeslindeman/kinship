{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc98b538-a2a0-487f-be05-8f7ca4b8b164",
   "metadata": {},
   "outputs": [],
   "source": [
    "# inputs\n",
    "natural_language_file = '../kinship_dutch.xlsx'\n",
    "natural_language_name = 'dutch'\n",
    "emerged_languages_files = {\n",
    "    '100-distractor': 'results/uniform42/evaluation.csv',\n",
    "}\n",
    "EGO = 'Alice'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "340b4270-b3ed-4dc7-a968-4a8e46f148d2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# node mapping\n",
    "NODES = [\n",
    "    'MM', 'MF', 'MZy', 'MBy', 'M', 'MZe', 'MBe',\n",
    "    'FM', 'FF', 'FZy', 'FBy', 'F', 'FZe', 'FBe',\n",
    "    'Zy', 'By', 'Ego', 'Ze', 'Be', 'ZyD', 'ZyS',\n",
    "    'ByD', 'ByS', 'D', 'S', 'ZeD', 'ZeS', 'BeD', 'BeS',\n",
    "    'DD', 'DS', 'SD', 'SS'\n",
    "]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67cfd75f-096d-489b-b625-ea0e758e5a24",
   "metadata": {},
   "source": [
    "## Complexity, information loss and accuracy for Dutch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "af99b506-ae44-4f22-93d2-d9cbe0828f0b",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "\n",
    "df = pd.read_excel(natural_language_file)\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd496436-abbd-4bf7-90d7-4af3e86802ae",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pandas import isna\n",
    "\n",
    "counts = []\n",
    "all_u = set()\n",
    "all_w = set()\n",
    "\n",
    "# collect counts\n",
    "for _, row in df.iterrows():\n",
    "    w = row['Word']\n",
    "    us = row['Target group']\n",
    "    if isna(us):\n",
    "        counts[-1][2] += row['Count']\n",
    "        continue # us = counts[-1][0] -- discard synonyms \n",
    "    else: \n",
    "        us = [u.strip() for u in us.split(',')]\n",
    "    counts.append([us, row['Word'], row['Count']])\n",
    "    all_u.update(set(us))\n",
    "    all_w.add(w)\n",
    "    \n",
    "print('counts')\n",
    "display(counts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3f64b157-809d-47c9-83cc-8ccab031aa24",
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict\n",
    "from itertools import product\n",
    "from math import log2\n",
    "\n",
    "\n",
    "def estimate_prob_given_count(counts):\n",
    "    # collect counts with conditions\n",
    "    count_u = defaultdict(lambda: 1e-10)  # for p(u)\n",
    "    count_w_u = defaultdict(lambda: defaultdict(lambda: 1e-10))  # for p(w|u)\n",
    "    count_u_w = defaultdict(lambda: defaultdict(lambda: 1e-10))  # for p(u|w)\n",
    "    \n",
    "    for us, w, c in counts:\n",
    "        for u in us:\n",
    "            count_u[u] += c / len(us)\n",
    "            count_w_u[u][w] += 1\n",
    "            count_u_w[w][u] += c / len(us)\n",
    "    \n",
    "    # estimate prob p(u), p(w|u), p(u|w)\n",
    "    p_u = defaultdict(lambda: 1e-10, {u:count_u[u] / sum(count_u.values()) for u in count_u.keys()})\n",
    "    \n",
    "    p_w_u = defaultdict(lambda: defaultdict(lambda: 1e-10), {\n",
    "        u:defaultdict(lambda: 1e-10, {\n",
    "            w:count_w_u[u][w] / sum(count_w_u[u].values())\n",
    "            for w in count_w_u[u].keys()\n",
    "        })\n",
    "        for u in count_w_u.keys()})\n",
    "    \n",
    "    p_u_w = defaultdict(lambda: defaultdict(lambda: 1e-10), {\n",
    "        w:defaultdict(lambda: 1e-10, {\n",
    "            u:count_u_w[w][u] / sum(count_u_w[w].values())\n",
    "            for u in count_u_w[w].keys()\n",
    "        })\n",
    "        for w in count_u_w.keys()})\n",
    "    \n",
    "    return p_u, p_w_u, p_u_w\n",
    "\n",
    "\n",
    "def compute_complexity_infoloss_accuracy(\n",
    "    all_u, all_w, \n",
    "    p_u, \n",
    "    p_sender_w_u, p_sender_u_w, \n",
    "    p_receiver_u_w\n",
    "):\n",
    "    complexity = 0  # I(W,U) = sum_u_w p(u) x p(w|u) x log2( p(u|w) / p(u) ) \n",
    "    for u, w in product(all_u, all_w):\n",
    "        complexity += (\n",
    "            p_u[u] * \n",
    "            p_sender_w_u[u][w] * \n",
    "            log2(p_sender_u_w[w][u] / p_u[u])\n",
    "        )\n",
    "        \n",
    "    info_loss = 0  # -sum_u_w p(u) x p(w|u) x log2(p(u|w))\n",
    "    for u, w in product(all_u, all_w):\n",
    "        info_loss += -p_u[u] * p_sender_w_u[u][w] * log2(p_receiver_u_w[w][u])\n",
    "    \n",
    "    acc = 0  # \n",
    "    for u, w in product(all_u, all_w): \n",
    "        acc += p_u[u] * p_sender_w_u[u][w] * p_receiver_u_w[w][u]\n",
    "    \n",
    "    return {\n",
    "        'complexity': complexity,\n",
    "        'info loss': info_loss,\n",
    "        'accuracy': acc\n",
    "    }\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ed4d32eb-cafa-4c7c-80e3-1a88974b8ca8",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_u, p_w_u, p_u_w = estimate_prob_given_count(counts)\n",
    "nl_metrics = compute_complexity_infoloss_accuracy(\n",
    "    all_u, all_w,\n",
    "    p_u, \n",
    "    p_w_u, \n",
    "    p_u_w, \n",
    "    p_u_w\n",
    ")\n",
    "display(nl_metrics)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65d998e9-4477-42be-a51d-0c41439009dd",
   "metadata": {},
   "source": [
    "## Complexity, information loss, accuracy for emergent language\n",
    "Using natural-language's need probabilities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5bd366c8-caf0-41d5-bbf3-2f2fc0ab456e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import numpy as np \n",
    "\n",
    "# read evalutaion file \n",
    "def process_one_file(path: str):\n",
    "    all_df = pd.read_csv(path)\n",
    "    eme_lang = []\n",
    "    \n",
    "    for epoch in range(0, 10000):\n",
    "        df = all_df[(all_df.Epoch == epoch) & (all_df[\"Ego Node\"] == EGO)]\n",
    "        if df.shape[0] == 0:\n",
    "            continue\n",
    "        \n",
    "        # estimate counts with weights \n",
    "        counts_sender = []\n",
    "        all_u = set()\n",
    "        all_w = set()\n",
    "        for _, row in df.iterrows(): \n",
    "            u, w = row['Target Node'], row['Message']\n",
    "            counts_sender.append(([u], w, p_u[u]))  # assume that a target has only one name\n",
    "            all_u.add(u)\n",
    "            all_w.add(w)\n",
    "        p_sender_u, p_sender_w_u, p_sender_u_w = estimate_prob_given_count(counts_sender)\n",
    "\n",
    "        # make sure that p_u == p_sender_u\n",
    "        for u, p in p_u.items():\n",
    "            assert abs(p_sender_u[u] - p) < 1e-5, f\"{[u, p, p_sender_u[u]]}\"\n",
    "        for u, p in p_sender_u.items():\n",
    "            assert abs(p_u[u] - p) < 1e-5, f\"{[u, p, p_u[u]]}\"\n",
    "\n",
    "        p_receiver_u_w = defaultdict(lambda: defaultdict(lambda: 1e-10))\n",
    "        outputs = {}\n",
    "        for _, row in df.iterrows(): \n",
    "            idx, u, w = row['Target Node Idx'], row['Target Node'], row['Message']\n",
    "            receiver_output = json.loads(row['Receiver Output'])\n",
    "            if isinstance(receiver_output[0], list):\n",
    "                receiver_output = receiver_output[0]\n",
    "\n",
    "            if w not in outputs:\n",
    "                outputs[w] = receiver_output\n",
    "            else: \n",
    "                assert all(o==r for o, r in zip(outputs[w], receiver_output)), \\\n",
    "                    [abs(o-r) for o, r in zip(outputs[w], receiver_output)]\n",
    "            \n",
    "            for uidx in range(len(receiver_output)):\n",
    "                p_receiver_u_w[w][NODES[uidx]] = max(1e-10, receiver_output[uidx])\n",
    "                        \n",
    "        eme_lang.append({\n",
    "                'epoch': epoch,\n",
    "                'metrics': compute_complexity_infoloss_accuracy(\n",
    "                    all_u, all_w,\n",
    "                    p_u, \n",
    "                    p_sender_w_u, \n",
    "                    p_sender_u_w, \n",
    "                    p_receiver_u_w\n",
    "                )\n",
    "            }\n",
    "        )\n",
    "    return eme_lang"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4630e075-1b9d-46d4-874e-541b688ade39",
   "metadata": {},
   "outputs": [],
   "source": [
    "el_metrics = {}\n",
    "for name, file in emerged_languages_files.items(): \n",
    "    el_metrics[name] = process_one_file(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37ba3a2f-8e15-4f97-b172-b601265ebe63",
   "metadata": {},
   "source": [
    "## Plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f17ce1ed-c6c5-4784-a37b-99b79e952fe9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from matplotlib import pyplot as plt\n",
    "\n",
    "# draw optimal boundary\n",
    "# information loss = - complexity + entropy(u)\n",
    "entropy_u = sum(-p_u[u] * log2(p_u[u]) for u in p_u.keys())\n",
    "plt.plot([0, entropy_u], [entropy_u, 0], '--b')\n",
    "\n",
    "# plot natural language\n",
    "plt.scatter([nl_metrics['complexity']], [nl_metrics['info loss']], color='green')\n",
    "\n",
    "# plot emerged language\n",
    "for el, color in zip(el_metrics.values(), ['orange', 'red', 'black', 'purple']):\n",
    "    plt.scatter(\n",
    "        [x['metrics']['complexity'] for x in el], \n",
    "        [x['metrics']['info loss'] for x in el], \n",
    "        color=color\n",
    "    )\n",
    "\n",
    "for el in el_metrics.values():\n",
    "    for i in range(len(el)):\n",
    "        l1, l2 = el[i-1]['metrics'], el[i]['metrics']\n",
    "        if i > 0: \n",
    "            plt.arrow(\n",
    "                l1['complexity'], l1['info loss'], \n",
    "                l2['complexity'] - l1['complexity'],\n",
    "                l2['info loss'] - l1['info loss'],\n",
    "                shape='full', lw=0.1, length_includes_head=True, head_width=.05\n",
    "            )\n",
    "\n",
    "plt.xlabel('complexity (bit)')\n",
    "plt.ylabel('information loss (bit)')\n",
    "plt.legend(['optimal', natural_language_name] + list(el_metrics.keys()))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77b27827-61b6-493f-a9b5-66f4ae328e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot accuracy\n",
    "\n",
    "# plot natural language\n",
    "plt.axhline(y=nl_metrics['accuracy'], color='b', linestyle='--', linewidth=2)\n",
    "\n",
    "# plot emerged language\n",
    "for el, color in zip(el_metrics.values(), ['orange', 'red', 'black', 'purple']):\n",
    "    plt.plot(\n",
    "        [x['epoch'] for x in el], \n",
    "        [x['metrics']['accuracy'] for x in el], \n",
    "        color=color\n",
    "    )\n",
    "\n",
    "plt.xlabel('epoch')\n",
    "plt.ylabel('accuracy')\n",
    "plt.legend([natural_language_name] + list(el_metrics.keys()))\n",
    "plt.grid()\n",
    "plt.show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
